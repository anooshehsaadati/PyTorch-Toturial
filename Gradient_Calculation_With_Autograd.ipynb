{"cells":[{"cell_type":"markdown","metadata":{"id":"w1TkrBuwGIKc"},"source":["## **Import important Libraries**"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":346,"status":"ok","timestamp":1682692567794,"user":{"displayName":"anoosheh saadati","userId":"11815996025752346748"},"user_tz":-210},"id":"9MU5iYhN2ARN"},"outputs":[],"source":["import torch\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"354myiJ5GSfF"},"source":["### **Gradient with scalar value**"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1.1981, 0.7285, 0.6427], requires_grad=True)\n","tensor([2.0405, 0.7849, 2.1110], grad_fn=<AddBackward0>)\n","tensor(16.4373, grad_fn=<MeanBackward0>)\n","tensor([4.2641, 3.6380, 3.5236])\n"]}],"source":["x1 = torch.randn(3, requires_grad=True)\n","print(x1)\n","\n","x2 = x1 + 2\n","print(x2)\n","\n","x3 = x2*x2*2\n","x3 = x3.mean()\n","print(x3)\n","\n","x3.backward() # dx3/dx1\n","print(x1.grad)"]},{"cell_type":"markdown","metadata":{},"source":["### **Gradient with vector**"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([-0.3423,  0.6730, -0.4464], requires_grad=True)\n","tensor([1.6577, 2.6730, 1.5536], grad_fn=<AddBackward0>)\n","tensor([ 5.4962, 14.2904,  4.8274], grad_fn=<MulBackward0>)\n","tensor([6.6310e-01, 1.0692e+01, 6.2144e-03])\n"]}],"source":["x4 = torch.randn(3, requires_grad=True)\n","print(x4)\n","\n","x5 = x4 + 2\n","print(x5)\n","\n","x6 = x5*x5*2\n","print(x6)\n","\n","v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n","x6.backward(v) # dx6/dx4\n","print(x4.grad)"]},{"cell_type":"markdown","metadata":{},"source":["### **Prevent tracking the history of gradient**"]},{"cell_type":"markdown","metadata":{},"source":["- x.requires_grad_(False)\n","- x.detach()\n","- with torch.no_grad():"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([-0.6420, -0.1796,  0.0222], requires_grad=True)\n","tensor([-0.6420, -0.1796,  0.0222])\n"]}],"source":["x7 = torch.randn(3, requires_grad=True)\n","print(x7)\n","x7.requires_grad_(False)\n","print(x7)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([-0.9813, -0.2444,  0.5999], requires_grad=True)\n","tensor([-0.9813, -0.2444,  0.5999])\n"]}],"source":["x8 = torch.randn(3, requires_grad=True)\n","print(x8)\n","x9 = x8.detach()\n","print(x9)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([ 2.7206,  0.1196, -0.6959], requires_grad=True)\n","tensor([4.7206, 2.1196, 1.3041])\n"]}],"source":["x10 = torch.randn(3, requires_grad=True)\n","print(x10)\n","with torch.no_grad():\n","    x11 = x10 + 2\n","    print(x11)"]},{"cell_type":"markdown","metadata":{},"source":["### **Simple Training**\n","- before next itteration and optimization step, we must empty the gradients. this is important"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([3., 3., 3., 3.])\n","tensor([3., 3., 3., 3.])\n","tensor([3., 3., 3., 3.])\n"]}],"source":["weights = torch.ones(4, requires_grad=True)\n","for epoch in range(3):\n","    model_output = (weights*3).sum()\n","    model_output.backward()\n","    print(weights.grad)\n","\n","    # before next itteration and optimization step we must empty gradient\n","    weights.grad.zero_()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM8qNaRLj6Oa+kRXHoB41vI","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
